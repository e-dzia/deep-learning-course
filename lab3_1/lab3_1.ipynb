{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=2000)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class LSTMModel(tf.keras.Model):\n",
    "    def __init__(self, h_size=8):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.num_words = 2000\n",
    "        self.h_size = h_size\n",
    "        \n",
    "        self.w_f = self.add_weight()\n",
    "        self.b_f = self.add_weight()\n",
    "        \n",
    "        self.w_i = self.add_weight()\n",
    "        self.b_i = self.add_weight()\n",
    "        \n",
    "        self.w_c = self.add_weight()\n",
    "        self.b_c = self.add_weight()\n",
    "        \n",
    "        self.w_o = self.add_weight()\n",
    "        self.b_o = self.add_weight()\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(units=2, activation='softmax')\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        print(\"x shape\", x.shape)\n",
    "        \n",
    "        h = tf.zeros(shape=(self.h_size, len(x)))\n",
    "        c = tf.zeros(shape=(self.h_size, len(x)))\n",
    "        \n",
    "        print(h.shape)\n",
    "        print(c.shape)\n",
    "        \n",
    "        for i in range(1):\n",
    "            print(\"i: \", i)\n",
    "            print(\"x[i]: \", x[i])\n",
    "            # print(x[:, i])\n",
    "            print(x[:, :, i])\n",
    "            h, c = self.lstm_cell(x[i], h, c)\n",
    "        \n",
    "        x = self.dense(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def lstm_cell(self, x_t, h_prev, c_prev):\n",
    "        x_t = tf.transpose(x_t)\n",
    "        \n",
    "        f = tf.sigmoid(tf.matmul(self.w_f, tf.concat(h_prev, x_t)) + self.b_f)\n",
    "        i = tf.sigmoid(tf.matmul(self.w_i, tf.concat(h_prev, x_t)) + self.b_i)\n",
    "        z = tf.tanh(tf.matmul(self.w_c, tf.concat(h_prev, x_t)) + self.b_c)\n",
    "        c = f * c_prev + i * z\n",
    "        \n",
    "        o = tf.sigmoid(tf.matmul(self.w_o, tf.concat(h_prev, x_t)) + self.b_o)\n",
    "        h = o * tf.tanh(c)\n",
    "        \n",
    "        return h, c\n",
    "\n",
    "    def fit(self, **kwargs):\n",
    "        self.epoch_loss = []\n",
    "        self.epoch_accuracy = []\n",
    "        self.epoch_fscore = []\n",
    "        self.epoch_time = []\n",
    "\n",
    "        x_train, y_train, epochs, batch_size, cost_func, learning_coef, optimizer = self._get_params(**kwargs)\n",
    "\n",
    "        weight_decay = kwargs.get('weight_decay', None)\n",
    "        if weight_decay is not None:\n",
    "            optimizer = optimizer(weight_decay=weight_decay, learning_rate=learning_coef)\n",
    "        else:\n",
    "            optimizer = optimizer(learning_rate=learning_coef)\n",
    "        train_dataset = self._extract_train_dataset(x_train, y_train, batch_size)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            start = datetime.datetime.now()\n",
    "            epoch_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "            \n",
    "            print(\"epoch: \", epoch)\n",
    "            \n",
    "            for x, y in train_dataset:\n",
    "                print(\"x shape: \", x.shape)\n",
    "                print(x)\n",
    "                print(\"y shape: \", y.shape)\n",
    "                print(y)\n",
    "                x_onehot = []\n",
    "                for x_el in x:\n",
    "                    x_onehot.append(tf.dtypes.cast(\n",
    "                        tf.keras.utils.to_categorical(x_el, num_classes=self.num_words), tf.int32))\n",
    "                \n",
    "                x = np.array(x_onehot)\n",
    "                \n",
    "                # x = tf.keras.utils.to_categorical(x, num_classes=self.num_words)\n",
    "                print(\"type(x): \", type(x))\n",
    "                # x = tf.dtypes.cast(x, tf.int32)\n",
    "                print(\"len(x): \", len(x))\n",
    "                \n",
    "                print(\"x shape: \", x.shape)\n",
    "                print(\"x[0] shape: \", x[0].shape)\n",
    "                print(\"x[0][0] shape: \", x[0][0].shape)\n",
    "                \n",
    "                y = tf.dtypes.cast(y, tf.float32)\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                    logits = self.call(x, training=True)\n",
    "                    loss_value = cost_func(y, logits)\n",
    "\n",
    "                grads = tape.gradient(loss_value, self.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "\n",
    "                epoch_acc.update_state(y, logits)\n",
    "                self.epoch_loss.append(loss_value)\n",
    "\n",
    "            end = datetime.datetime.now()\n",
    "            diff = end - start\n",
    "            self.epoch_time.append(diff.total_seconds())\n",
    "\n",
    "            self.epoch_accuracy.append(epoch_acc.result().numpy())\n",
    "\n",
    "            print(\"Epoch {:02d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,\n",
    "                                                                        self.epoch_loss[-1], \n",
    "                                                                        self.epoch_accuracy[-1]))\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Predicts outputs based on inputs (x).\"\"\"\n",
    "        x_dataset = self._extract_test_dataset(x, batch_size=1000)\n",
    "        prediction = []\n",
    "        for x in x_dataset:\n",
    "            logits = self.call(x)\n",
    "            prediction.extend(tf.argmax(logits, axis=1, output_type=tf.int32))\n",
    "        return prediction\n",
    "\n",
    "    def _extract_train_dataset(self, x_train, y_train, batch_size):\n",
    "        x_train = self._preprocess_x(x_train)\n",
    "        max_length = max(len(lst) for lst in x_train)\n",
    "        \n",
    "        #train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "        \n",
    "        # x_train = tf.data.Dataset.from_generator(lambda: x_train, tf.int32, output_shapes=[None])\n",
    "        # y_train = tf.data.Dataset.from_tensors((y_train))\n",
    "        # train_dataset = tf.data.Dataset.zip((x_train, y_train))\n",
    "        # for x, y in train_dataset:\n",
    "        #     print(x, y)\n",
    "        x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                              padding='post')\n",
    "        # x_train = tf.ragged.constant(x_train)\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "        train_dataset = train_dataset.shuffle(buffer_size=50000).batch(batch_size=batch_size)\n",
    "            # padded_batch(\n",
    "            # batch_size=batch_size, padded_shapes=[None, None, None])\n",
    "        print(\"train dataset: \", train_dataset)\n",
    "        return train_dataset\n",
    "        # return zip(x_train, y_train)\n",
    "\n",
    "    def _extract_test_dataset(self, x_train, batch_size):\n",
    "        x_train = self._preprocess_x(x_train)\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "        train_dataset = train_dataset.batch(batch_size=batch_size)\n",
    "        return train_dataset\n",
    "        # return x_train\n",
    "        \n",
    "    def _preprocess_x(self, x_set):\n",
    "        # x_onehot.append(tf.keras.utils.to_categorical(x, num_classes=2000))\n",
    "        return x_set\n",
    "\n",
    "    def _get_params(self, **kwargs):\n",
    "        x_train = kwargs.get('x', None)\n",
    "        y_train = kwargs.get('y', None)\n",
    "        epochs = kwargs.get('epochs', 10)\n",
    "        batch_size = kwargs.get('batch_size', 100)\n",
    "        cost_func = kwargs.get('cost_func', tf.keras.losses.SparseCategoricalCrossentropy())\n",
    "        learning_coef = kwargs.get('learning_coef', 0.001)\n",
    "        optimizer = kwargs.get('optimizer', tf.keras.optimizers.Adam)\n",
    "        return x_train, y_train, epochs, batch_size, cost_func, learning_coef, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "learning_coef = 0.01  # 0.01\n",
    "epochs = 10\n",
    "cost_func = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.SGD\n",
    "\n",
    "\n",
    "def test_params_base(batch_size, learning_coef, epochs, cost_func, optimizer,\n",
    "                     kernel_size, pool_size, pooling, weight_decay=None,\n",
    "                     x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test):\n",
    "    model = LSTMModel()\n",
    "    \n",
    "    print(\"Started fitting\")\n",
    "\n",
    "    model.fit(x=x_train, y=y_train, batch_size=batch_size, learning_coef=learning_coef, \n",
    "             epochs=epochs, cost_func=cost_func, optimizer=optimizer, weight_decay=weight_decay)\n",
    "    \n",
    "    y_pred = model.predict(x_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    loss = model.epoch_loss[-1]\n",
    "    return model, acc, f1, loss.numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Started fitting\n",
      "train dataset:  <BatchDataset shapes: ((None, 2494), (None,)), types: (tf.int32, tf.int64)>\nepoch:  0\n",
      "x shape:  (100, 2494)\ntf.Tensor(\n[[   1   13  296 ...    0    0    0]\n [   1   13   69 ...    0    0    0]\n [   1   51  126 ...    0    0    0]\n ...\n [   1 1301   73 ...    0    0    0]\n [   1   14   20 ...    0    0    0]\n [   1   99   78 ...    0    0    0]], shape=(100, 2494), dtype=int32)\ny shape:  (100,)\ntf.Tensor(\n[1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 1\n 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1\n 0 1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 1 1 1 0 0], shape=(100,), dtype=int64)\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-54a65d4f3253>\u001b[0m in \u001b[0;36mtest_params_base\u001b[0;34m(batch_size, learning_coef, epochs, cost_func, optimizer, kernel_size, pool_size, pooling, weight_decay, x_train, y_train, x_test, y_test)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     model.fit(x=x_train, y=y_train, batch_size=batch_size, learning_coef=learning_coef, \n\u001b[0;32m---> 16\u001b[0;31m              epochs=epochs, cost_func=cost_func, optimizer=optimizer, weight_decay=weight_decay)\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-0f1cf4f7f6a6>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mx_el\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     x_onehot.append(tf.dtypes.cast(\n\u001b[0;32m---> 84\u001b[0;31m                         tf.keras.utils.to_categorical(x_el, num_classes=self.num_words), tf.int32))\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_onehot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PWr/venv/lib/python3.6/site-packages/tensorflow_core/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PWr/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mcast\u001b[0;34m(x, dtype, name)\u001b[0m\n\u001b[1;32m    702\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Casting complex to real discards imaginary part.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PWr/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mcast\u001b[0;34m(x, DstT, Truncate, name)\u001b[0m\n\u001b[1;32m   2210\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2211\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2212\u001b[0;31m       \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2213\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m   \u001b[0mDstT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDstT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DstT\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PWr/venv/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[2494,2000] and type int32 on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Cast] name: Cast/"
     ],
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[2494,2000] and type int32 on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Cast] name: Cast/",
     "output_type": "error"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 100\n",
    "learning_coef = 0.001\n",
    "epochs = 1\n",
    "cost_func = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam\n",
    "kernel_size = (3, 3)\n",
    "pool_size = (4, 4)\n",
    "pooling = tf.keras.layers.MaxPool2D\n",
    "\n",
    "\n",
    "results = test_params_base(batch_size=batch_size, learning_coef=learning_coef, \n",
    "                           epochs=epochs, cost_func=cost_func, \n",
    "                           optimizer=optimizer, kernel_size=kernel_size, \n",
    "                           pool_size=pool_size, pooling=pooling)\n",
    "model, acc, f1, loss = results\n",
    "    \n",
    "res_df = pd.DataFrame(columns=['batch_size', 'learning_coef', 'epochs', \n",
    "                               'cost_func', 'optimizer', 'kernel_size', \n",
    "                               'pool_size', 'pooling', 'acc', 'f1', 'loss', 'time'])\n",
    "res_df = res_df.append({'batch_size': batch_size, 'learning_coef': learning_coef,\n",
    "                        'epochs': epochs, 'cost_func': type(cost_func).__name__, \n",
    "                        'optimizer': optimizer.__name__, \n",
    "                        'kernel_size': kernel_size, 'pool_size': pool_size, \n",
    "                        'pooling': pooling.__name__, \n",
    "                        'acc': acc, 'f1': f1, 'loss': loss, \n",
    "                        'time': sum(model.epoch_time)}, \n",
    "                       ignore_index=True)\n",
    "\n",
    "print('Time: {}'.format(sum(model.epoch_time)))\n",
    "\n",
    "with open('results.csv', 'a') as f:\n",
    "    res_df.to_csv(f, header=False)\n",
    "    \n",
    "now = datetime.datetime.now()\n",
    "now = str(now).replace(':', '_').replace(' ', '_').replace('.', '_').replace('-', '_')\n",
    "tf.saved_model.save(model, \"model_{}_{}\".format(now, acc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# x_train_onehot = []\n",
    "# for x in x_train[:200]:\n",
    "#     x_train_onehot.append(tf.keras.utils.to_categorical(x, num_classes=2000))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# len(x_train_onehot)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "data": {
      "text/plain": "(100, 2494, 2000)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 105
    }
   ],
   "source": [
    "tmp = tf.keras.preprocessing.sequence.pad_sequences(x_train, padding='post')\n",
    "tmp = tf.keras.utils.to_categorical(tmp[:100], num_classes=2000)\n",
    "tmp.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "data": {
      "text/plain": "(3, 1, 4)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 106
    }
   ],
   "source": [
    "# tmp = np.array([[[1, 2, 3, 4],[5, 6, 7, 8]],\n",
    "#                 [[9, 10, 11, 12],[13, 14, 15, 16]],\n",
    "#                 [[17, 18, 19, 20],[21, 22, 23, 24]]])\n",
    "tmp = np.array([[[1, 2, 3, 4]],\n",
    "                [[9, 10, 11, 12]],\n",
    "                [[17, 18, 19, 20]]])\n",
    "tmp.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "(3, 4)\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "array([[ 1,  2,  3,  4],\n       [ 9, 10, 11, 12],\n       [17, 18, 19, 20]])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 108
    }
   ],
   "source": [
    "t1 = tmp[:, 0]\n",
    "print(t1.shape)\n",
    "t1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "# t2 = np.array([[[10, 20, 30, 40],[50, 60, 70, 80]],\n",
    "#                 [[90, 100, 110, 120],[130, 140, 150, 160]],\n",
    "#                 [[170, 180, 190, 200],[210, 220, 230, 240]]])\n",
    "t2 = np.array([[[10, 20, 30, 40]],\n",
    "                [[90, 100, 110, 120]],\n",
    "                [[170, 180, 190, 200]]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=29189, shape=(8, 1, 3), dtype=int64, numpy=\narray([[[  1,   9,  17]],\n\n       [[  2,  10,  18]],\n\n       [[  3,  11,  19]],\n\n       [[  4,  12,  20]],\n\n       [[ 10,  90, 170]],\n\n       [[ 20, 100, 180]],\n\n       [[ 30, 110, 190]],\n\n       [[ 40, 120, 200]]])>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 112
    }
   ],
   "source": [
    "tmp2 = tf.transpose(tmp)\n",
    "t3 = tf.transpose(t2)\n",
    "tf.concat([tmp2, t3], 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=29197, shape=(4, 3), dtype=int32, numpy=\narray([[ 90, 120, 150],\n       [190, 260, 330],\n       [290, 400, 510],\n       [390, 540, 690]], dtype=int32)>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 116
    }
   ],
   "source": [
    "tmp = [[10, 20], [30, 40], [50, 60], [70, 80]]\n",
    "tmp2 = [[1, 2, 3], [4, 5, 6]]\n",
    "tf.matmul(tmp, tmp2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}