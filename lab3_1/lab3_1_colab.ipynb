{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab3_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ivk2xVlK_G0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d4e1dd74-a818-4fc0-889f-aecafc2d408e"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import files\n",
        "DATADIR = \"/content/gdrive/My Drive/Colab Notebooks/DL/\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GW5L7J1HLCeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=2000)\n",
        "x_train, y_train = x_train[:5000], y_train[:5000]\n",
        "x_test, y_test = x_test[:2000], y_test[:2000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbCJeLxlLaOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_params_lstm(batch_size, learning_coef, epochs, cost_func, optimizer,\n",
        "                     x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test):\n",
        "    model = LSTMModel()\n",
        "    \n",
        "    print(\"Started fitting\")\n",
        "\n",
        "    model.fit(x=x_train, y=y_train, batch_size=batch_size, learning_coef=learning_coef, \n",
        "             epochs=epochs, cost_func=cost_func, optimizer=optimizer)\n",
        "    \n",
        "    y_pred = model.predict(x_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average='macro')\n",
        "    loss = model.epoch_loss[-1]\n",
        "    return model, acc, f1, loss.numpy()\n",
        "\n",
        "\n",
        "def test_params_rnn(batch_size, learning_coef, epochs, cost_func, optimizer,\n",
        "                     x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test):\n",
        "    model = RNNMModel()\n",
        "    \n",
        "    print(\"Started fitting\")\n",
        "\n",
        "    model.fit(x=x_train, y=y_train, batch_size=batch_size, learning_coef=learning_coef, \n",
        "             epochs=epochs, cost_func=cost_func, optimizer=optimizer)\n",
        "    \n",
        "    y_pred = model.predict(x_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average='macro')\n",
        "    loss = model.epoch_loss[-1]\n",
        "    return model, acc, f1, loss.numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktufZszNLENZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMModel(tf.keras.Model):\n",
        "    def __init__(self, h_size=8):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        \n",
        "        self.num_words = 2000\n",
        "        self.h_size = h_size\n",
        "        \n",
        "        self.w_f = self.add_weight(shape=(self.h_size, self.h_size + self.num_words), \n",
        "                                   initializer='truncated_normal', trainable=True)\n",
        "        self.b_f = self.add_weight(shape=(self.h_size, 1), \n",
        "                                   initializer='truncated_normal', trainable=True)\n",
        "        \n",
        "        self.w_i = self.add_weight(shape=(self.h_size, self.h_size + self.num_words), \n",
        "                                   initializer='truncated_normal', trainable=True)\n",
        "        self.b_i = self.add_weight(shape=(self.h_size, 1), \n",
        "                                   initializer='truncated_normal', trainable=True)\n",
        "        \n",
        "        self.w_c = self.add_weight(shape=(self.h_size, self.h_size + self.num_words), \n",
        "                                   initializer='truncated_normal', trainable=True)\n",
        "        self.b_c = self.add_weight(shape=(self.h_size, 1), \n",
        "                                   initializer='truncated_normal', trainable=True)\n",
        "        \n",
        "        self.w_o = self.add_weight(shape=(self.h_size, self.h_size + self.num_words), \n",
        "                                   initializer='truncated_normal', trainable=True)\n",
        "        self.b_o = self.add_weight(shape=(self.h_size, 1), \n",
        "                                   initializer='truncated_normal', trainable=True)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(units=2, activation='softmax')\n",
        "\n",
        "    def __call__(self, x, training=False):\n",
        "        h = tf.zeros(shape=(self.h_size, len(x)))\n",
        "        c = tf.zeros(shape=(self.h_size, len(x)))\n",
        "        \n",
        "        for i in range(x.shape[1]):\n",
        "            h, c = self.lstm_cell(x[:, i], h, c)\n",
        "        \n",
        "        x = self.dense(tf.transpose(h))\n",
        "\n",
        "        return x\n",
        "    \n",
        "    def lstm_cell(self, x_t, h_prev, c_prev):\n",
        "        x_t = tf.transpose(x_t)\n",
        "\n",
        "        f = tf.sigmoid(tf.matmul(self.w_f, tf.concat([x_t, h_prev], 0)) + self.b_f)\n",
        "        i = tf.sigmoid(tf.matmul(self.w_i, tf.concat([x_t, h_prev], 0)) + self.b_i)\n",
        "        z = tf.tanh(tf.matmul(self.w_c, tf.concat([x_t, h_prev], 0)) + self.b_c)\n",
        "        c = f * c_prev + i * z\n",
        "        \n",
        "        o = tf.sigmoid(tf.matmul(self.w_o, tf.concat([x_t, h_prev], 0)) + self.b_o)\n",
        "        h = o * tf.tanh(c)\n",
        "        \n",
        "        return h, c\n",
        "\n",
        "    def fit(self, **kwargs):\n",
        "        self.epoch_loss = []\n",
        "        self.epoch_accuracy = []\n",
        "        self.epoch_fscore = []\n",
        "        self.epoch_time = []\n",
        "\n",
        "        x_train, y_train, epochs, batch_size, cost_func, learning_coef, optimizer = self._get_params(**kwargs)\n",
        "        \n",
        "        weight_decay = kwargs.get('weight_decay', None)\n",
        "        if weight_decay is not None:\n",
        "            optimizer = optimizer(weight_decay=weight_decay, learning_rate=learning_coef)\n",
        "        else:\n",
        "            optimizer = optimizer(learning_rate=learning_coef)\n",
        "        train_dataset = self._extract_train_dataset(x_train, y_train, batch_size)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            start = datetime.datetime.now()\n",
        "            epoch_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "            \n",
        "            print(\"epoch: \", epoch)\n",
        "            i = 0\n",
        "            for x, y in train_dataset:\n",
        "                print(i)\n",
        "                i += 1\n",
        "\n",
        "                x = tf.keras.utils.to_categorical(x, num_classes=self.num_words)\n",
        "                \n",
        "                with tf.GradientTape() as tape:\n",
        "                    logits = self.__call__(x, training=True)\n",
        "                    loss_value = cost_func(y, logits)\n",
        "\n",
        "                grads = tape.gradient(loss_value, self.trainable_variables)\n",
        "                optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
        "\n",
        "                epoch_acc.update_state(y, logits)\n",
        "                self.epoch_loss.append(loss_value)\n",
        "\n",
        "            end = datetime.datetime.now()\n",
        "            diff = end - start\n",
        "            self.epoch_time.append(diff.total_seconds())\n",
        "\n",
        "            self.epoch_accuracy.append(epoch_acc.result().numpy())\n",
        "\n",
        "            print(\"Epoch {:02d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,\n",
        "                                                                        self.epoch_loss[-1], \n",
        "                                                                        self.epoch_accuracy[-1]))\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\"Predicts outputs based on inputs (x).\"\"\"\n",
        "        x_dataset = self._extract_test_dataset(x, batch_size=100)\n",
        "        prediction = []\n",
        "        for x in x_dataset:\n",
        "            x = tf.keras.utils.to_categorical(x, num_classes=self.num_words)\n",
        "            logits = self.__call__(x)\n",
        "            prediction.extend(tf.argmax(logits, axis=1, output_type=tf.int32))\n",
        "        return prediction\n",
        "\n",
        "    def _extract_train_dataset(self, x_train, y_train, batch_size):\n",
        "        y_train = tf.dtypes.cast(y_train, tf.float32)\n",
        "        x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
        "                                                              padding='post')\n",
        "        train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "        train_dataset = train_dataset.shuffle(buffer_size=25000).batch(batch_size=batch_size)\n",
        "        return train_dataset\n",
        "\n",
        "    def _extract_test_dataset(self, x_train, batch_size):\n",
        "        x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
        "                                                              padding='post')\n",
        "\n",
        "        train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
        "        train_dataset = train_dataset.batch(batch_size=batch_size)\n",
        "        return train_dataset\n",
        "\n",
        "    def _get_params(self, **kwargs):\n",
        "        x_train = kwargs.get('x', None)\n",
        "        y_train = kwargs.get('y', None)\n",
        "        epochs = kwargs.get('epochs', 10)\n",
        "        batch_size = kwargs.get('batch_size', 100)\n",
        "        cost_func = kwargs.get('cost_func', tf.keras.losses.SparseCategoricalCrossentropy())\n",
        "        learning_coef = kwargs.get('learning_coef', 0.001)\n",
        "        optimizer = kwargs.get('optimizer', tf.keras.optimizers.Adam)\n",
        "        return x_train, y_train, epochs, batch_size, cost_func, learning_coef, optimizer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAECUX90TFE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNMModel(LSTMModel):\n",
        "    def __init__(self, h_size=8):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        \n",
        "        self.num_words = 2000\n",
        "        self.h_size = h_size\n",
        "        \n",
        "        self.w_o = self.add_weight(shape=(self.h_size, self.h_size + self.num_words), \n",
        "                                   initializer='truncated_normal', trainable=True)\n",
        "        self.b_o = self.add_weight(shape=(self.h_size, 1), \n",
        "                                   initializer='truncated_normal', trainable=True)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(units=2, activation='softmax')\n",
        "\n",
        "    def __call__(self, x, training=False):\n",
        "        h = tf.zeros(shape=(self.h_size, len(x)))\n",
        "        \n",
        "        for i in range(x.shape[1]):\n",
        "            h = self.rnn_cell(x[:, i], h)\n",
        "        \n",
        "        x = self.dense(tf.transpose(h))\n",
        "\n",
        "        return x\n",
        "    \n",
        "    def rnn_cell(self, x_t, h_prev):\n",
        "        x_t = tf.transpose(x_t)\n",
        "        c = tf.tanh(tf.matmul(self.w_o, tf.concat([x_t, h_prev], 0)) + self.b_o)\n",
        "        return c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXcbYfaVilVk",
        "colab_type": "text"
      },
      "source": [
        "**Test hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPTHASY_WzrH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "9c74f228-d82d-4200-b207-339a083ed2d5"
      },
      "source": [
        "%%time\n",
        "\n",
        "batch_size = 100\n",
        "learning_coef = 0.001\n",
        "epochs = 3\n",
        "cost_func = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam\n",
        "\n",
        "experiments_epochs = [1, 3]\n",
        "experiments_learning_coef = [0.001, 0.005]\n",
        "experiments_optimizer = [tf.keras.optimizers.SGD, tf.keras.optimizers.Adam]\n",
        "\n",
        "for epochs in experiments_epochs:\n",
        "  for learning_coef in experiments_learning_coef:\n",
        "    for optimizer in experiments_optimizer:\n",
        "      results = test_params_lstm(batch_size=batch_size, learning_coef=learning_coef, \n",
        "                                epochs=epochs, cost_func=cost_func, \n",
        "                                optimizer=optimizer)\n",
        "      model, acc, f1, loss = results\n",
        "          \n",
        "      res_df = pd.DataFrame(columns=['model', 'batch_size', 'learning_coef', 'epochs', \n",
        "                                    'cost_func', 'optimizer', 'acc', 'f1', 'loss', 'time'])\n",
        "      res_df = res_df.append({'model': 'LSTM', 'batch_size': batch_size, 'learning_coef': learning_coef,\n",
        "                              'epochs': epochs, 'cost_func': type(cost_func).__name__, \n",
        "                              'optimizer': optimizer.__name__, \n",
        "                              'acc': acc, 'f1': f1, 'loss': loss, \n",
        "                              'time': sum(model.epoch_time)}, \n",
        "                            ignore_index=True)\n",
        "\n",
        "      print('Time: {}'.format(sum(model.epoch_time)))\n",
        "\n",
        "      with open(DATADIR + 'results.csv', 'a') as f:\n",
        "        res_df.to_csv(f, header=False)\n",
        "\n",
        "      # results = test_params_rnn(batch_size=batch_size, learning_coef=learning_coef, \n",
        "      #                           epochs=epochs, cost_func=cost_func, \n",
        "      #                           optimizer=optimizer)\n",
        "      # model, acc, f1, loss = results\n",
        "          \n",
        "      # res_df = pd.DataFrame(columns=['model', 'batch_size', 'learning_coef', 'epochs', \n",
        "      #                               'cost_func', 'optimizer', 'acc', 'f1', 'loss', 'time'])\n",
        "      # res_df = res_df.append({'model': 'RNN', 'batch_size': batch_size, 'learning_coef': learning_coef,\n",
        "      #                         'epochs': epochs, 'cost_func': type(cost_func).__name__, \n",
        "      #                         'optimizer': optimizer.__name__, \n",
        "      #                         'acc': acc, 'f1': f1, 'loss': loss, \n",
        "      #                         'time': sum(model.epoch_time)}, \n",
        "      #                       ignore_index=True)\n",
        "\n",
        "      # print('Time: {}'.format(sum(model.epoch_time)))\n",
        "\n",
        "      # with open(DATADIR + 'results.csv', 'a') as f:\n",
        "      #   res_df.to_csv(f, header=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Started fitting\n",
            "epoch:  0\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9zOyjjFkL3r",
        "colab_type": "text"
      },
      "source": [
        "Other"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3gBGsnGLcab",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "757ccf18-3fb9-4e03-f887-1262e92389f9"
      },
      "source": [
        "%%time\n",
        "\n",
        "batch_size = 100\n",
        "learning_coef = 0.001\n",
        "epochs = 1\n",
        "cost_func = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam\n",
        "\n",
        "\n",
        "results = test_params_lstm(batch_size=batch_size, learning_coef=learning_coef, \n",
        "                           epochs=epochs, cost_func=cost_func, \n",
        "                           optimizer=optimizer)\n",
        "model, acc, f1, loss = results\n",
        "    \n",
        "res_df = pd.DataFrame(columns=['model', 'batch_size', 'learning_coef', 'epochs', \n",
        "                               'cost_func', 'optimizer', 'acc', 'f1', 'loss', 'time'])\n",
        "res_df = res_df.append({'model': 'LSTM', 'batch_size': batch_size, 'learning_coef': learning_coef,\n",
        "                        'epochs': epochs, 'cost_func': type(cost_func).__name__, \n",
        "                        'optimizer': optimizer.__name__, \n",
        "                        'acc': acc, 'f1': f1, 'loss': loss, \n",
        "                        'time': sum(model.epoch_time)}, \n",
        "                       ignore_index=True)\n",
        "\n",
        "print('Time: {}'.format(sum(model.epoch_time)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Started fitting\n",
            "epoch:  0\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "Epoch 00: Loss: 0.695, Accuracy: 50.440%\n",
            "Time: 540.131955\n",
            "CPU times: user 9min 50s, sys: 9.13 s, total: 9min 59s\n",
            "Wall time: 10min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPKZMfZ-VseX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        },
        "outputId": "772230de-a9df-4b05-d2d3-3cfb3b7f8cdf"
      },
      "source": [
        "%%time\n",
        "\n",
        "batch_size = 100\n",
        "learning_coef = 0.001\n",
        "epochs = 1\n",
        "cost_func = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam\n",
        "\n",
        "\n",
        "results = test_params_rnn(batch_size=batch_size, learning_coef=learning_coef, \n",
        "                           epochs=epochs, cost_func=cost_func, \n",
        "                           optimizer=optimizer)\n",
        "model, acc, f1, loss = results\n",
        "    \n",
        "res_df = pd.DataFrame(columns=['model', 'batch_size', 'learning_coef', 'epochs', \n",
        "                               'cost_func', 'optimizer', 'acc', 'f1', 'loss', 'time'])\n",
        "res_df = res_df.append({'model': 'RNN', 'batch_size': batch_size, 'learning_coef': learning_coef,\n",
        "                        'epochs': epochs, 'cost_func': type(cost_func).__name__, \n",
        "                        'optimizer': optimizer.__name__, \n",
        "                        'acc': acc, 'f1': f1, 'loss': loss, \n",
        "                        'time': sum(model.epoch_time)}, \n",
        "                       ignore_index=True)\n",
        "\n",
        "print('Time: {}'.format(sum(model.epoch_time)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Started fitting\n",
            "epoch:  0\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "Epoch 00: Loss: 0.696, Accuracy: 50.920%\n",
            "Time: 186.619398\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}